{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise-4: Model Comparison [30 points]\n",
    "\n",
    "This notebook provides a comprehensive comparison of different machine learning models, focusing on Decision Trees and Support Vector Machines from the previous exercises, along with additional baseline models.\n",
    "\n",
    "## Learning Objectives\n",
    "- Systematic comparison of multiple machine learning algorithms\n",
    "- Understanding model selection criteria and evaluation metrics\n",
    "- Analyzing trade-offs between different algorithms\n",
    "- Statistical significance testing for model comparisons\n",
    "- Creating comprehensive model evaluation reports\n",
    "- Making informed decisions about model selection\n",
    "\n",
    "## Instructions\n",
    "Complete the exercises below by implementing the required code in the designated cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for comprehensive model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data\n",
    "\n",
    "Load the data that was prepared in Exercise-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the preprocessed data from Exercise-1\n",
    "# Example:\n",
    "# X_train = pd.read_csv('../data/X_train_processed.csv')\n",
    "# X_test = pd.read_csv('../data/X_test_processed.csv')\n",
    "# y_train = pd.read_csv('../data/y_train.csv').squeeze()\n",
    "# y_test = pd.read_csv('../data/y_test.csv').squeeze()\n",
    "\n",
    "# TODO: Display the shape of the datasets\n",
    "# TODO: Determine if this is a classification or regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Models for Comparison\n",
    "\n",
    "Define a comprehensive set of models to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define models for comparison\n",
    "# For classification:\n",
    "# models = {\n",
    "#     'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "#     'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
    "#     'SVM (Linear)': SVC(kernel='linear', random_state=42),\n",
    "#     'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "#     'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "#     'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "#     'Naive Bayes': GaussianNB()\n",
    "# }\n",
    "\n",
    "# For regression:\n",
    "# models = {\n",
    "#     'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "#     'SVM (RBF)': SVR(kernel='rbf'),\n",
    "#     'SVM (Linear)': SVR(kernel='linear'),\n",
    "#     'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "#     'Linear Regression': LinearRegression(),\n",
    "#     'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=5)\n",
    "# }\n",
    "\n",
    "# TODO: Choose appropriate models based on your problem type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Model Training and Evaluation\n",
    "\n",
    "Train all models and collect basic performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train all models and collect basic metrics\n",
    "# results = {}\n",
    "# training_times = {}\n",
    "# prediction_times = {}\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     print(f\"Training {name}...\")\n",
    "#     \n",
    "#     # Measure training time\n",
    "#     start_time = time.time()\n",
    "#     model.fit(X_train, y_train)\n",
    "#     training_time = time.time() - start_time\n",
    "#     \n",
    "#     # Measure prediction time\n",
    "#     start_time = time.time()\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     prediction_time = time.time() - start_time\n",
    "#     \n",
    "#     # Calculate metrics\n",
    "#     if hasattr(model, 'predict_proba'):  # Classification\n",
    "#         accuracy = accuracy_score(y_test, y_pred)\n",
    "#         results[name] = {\n",
    "#             'accuracy': accuracy,\n",
    "#             'predictions': y_pred\n",
    "#         }\n",
    "#     else:  # Regression\n",
    "#         mse = mean_squared_error(y_test, y_pred)\n",
    "#         mae = mean_absolute_error(y_test, y_pred)\n",
    "#         r2 = r2_score(y_test, y_pred)\n",
    "#         results[name] = {\n",
    "#             'mse': mse,\n",
    "#             'mae': mae,\n",
    "#             'r2': r2,\n",
    "#             'predictions': y_pred\n",
    "#         }\n",
    "#     \n",
    "#     training_times[name] = training_time\n",
    "#     prediction_times[name] = prediction_time\n",
    "#     \n",
    "#     print(f\"  Training time: {training_time:.4f}s\")\n",
    "#     print(f\"  Prediction time: {prediction_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation Comparison\n",
    "\n",
    "Perform systematic cross-validation for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform cross-validation for all models\n",
    "# cv_results = {}\n",
    "# cv_folds = 5\n",
    "\n",
    "# # Choose appropriate cross-validation strategy\n",
    "# if problem_type == 'classification':\n",
    "#     cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "#     scoring = 'accuracy'\n",
    "# else:\n",
    "#     cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "#     scoring = 'r2'  # or 'neg_mean_squared_error'\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     print(f\"Cross-validating {name}...\")\n",
    "#     scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n",
    "#     cv_results[name] = {\n",
    "#         'scores': scores,\n",
    "#         'mean': scores.mean(),\n",
    "#         'std': scores.std()\n",
    "#     }\n",
    "#     print(f\"  CV Score: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Visualization\n",
    "\n",
    "Create comprehensive visualizations comparing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create performance comparison plots\n",
    "\n",
    "# Cross-validation scores comparison\n",
    "# cv_df = pd.DataFrame({\n",
    "#     'Model': list(cv_results.keys()),\n",
    "#     'Mean_Score': [cv_results[model]['mean'] for model in cv_results.keys()],\n",
    "#     'Std_Score': [cv_results[model]['std'] for model in cv_results.keys()]\n",
    "# })\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.errorbar(cv_df['Model'], cv_df['Mean_Score'], yerr=cv_df['Std_Score'], \n",
    "#              fmt='o', capsize=5, capthick=2, elinewidth=2, markersize=8)\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.title('Cross-Validation Performance Comparison')\n",
    "# plt.ylabel('Score')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Box plots for cross-validation scores\n",
    "# cv_scores_list = [cv_results[model]['scores'] for model in cv_results.keys()]\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.boxplot(cv_scores_list, labels=list(cv_results.keys()))\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.title('Cross-Validation Score Distribution')\n",
    "# plt.ylabel('Score')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Prediction Time Analysis\n",
    "\n",
    "Compare computational efficiency of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create time comparison plots\n",
    "# time_df = pd.DataFrame({\n",
    "#     'Model': list(training_times.keys()),\n",
    "#     'Training_Time': list(training_times.values()),\n",
    "#     'Prediction_Time': list(prediction_times.values())\n",
    "# })\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# # Training time comparison\n",
    "# ax1.bar(time_df['Model'], time_df['Training_Time'])\n",
    "# ax1.set_title('Training Time Comparison')\n",
    "# ax1.set_ylabel('Time (seconds)')\n",
    "# ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# # Prediction time comparison\n",
    "# ax2.bar(time_df['Model'], time_df['Prediction_Time'])\n",
    "# ax2.set_title('Prediction Time Comparison')\n",
    "# ax2.set_ylabel('Time (seconds)')\n",
    "# ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# TODO: Create a scatter plot of performance vs training time\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(time_df['Training_Time'], cv_df['Mean_Score'], s=100)\n",
    "# for i, model in enumerate(time_df['Model']):\n",
    "#     plt.annotate(model, (time_df['Training_Time'][i], cv_df['Mean_Score'][i]), \n",
    "#                  xytext=(5, 5), textcoords='offset points')\n",
    "# plt.xlabel('Training Time (seconds)')\n",
    "# plt.ylabel('Cross-Validation Score')\n",
    "# plt.title('Performance vs Training Time Trade-off')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Significance Testing\n",
    "\n",
    "Perform statistical tests to determine if performance differences are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform pairwise statistical significance tests\n",
    "# from scipy.stats import ttest_rel\n",
    "\n",
    "# # Get model names and their CV scores\n",
    "# model_names = list(cv_results.keys())\n",
    "# n_models = len(model_names)\n",
    "\n",
    "# # Create a matrix to store p-values\n",
    "# p_values = np.zeros((n_models, n_models))\n",
    "\n",
    "# for i in range(n_models):\n",
    "#     for j in range(n_models):\n",
    "#         if i != j:\n",
    "#             scores_i = cv_results[model_names[i]]['scores']\n",
    "#             scores_j = cv_results[model_names[j]]['scores']\n",
    "#             _, p_value = ttest_rel(scores_i, scores_j)\n",
    "#             p_values[i, j] = p_value\n",
    "\n",
    "# # Create a heatmap of p-values\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(p_values, \n",
    "#             xticklabels=model_names, \n",
    "#             yticklabels=model_names,\n",
    "#             annot=True, \n",
    "#             fmt='.3f', \n",
    "#             cmap='viridis',\n",
    "#             cbar_kws={'label': 'p-value'})\n",
    "# plt.title('Statistical Significance Test (p-values)\\nValues < 0.05 indicate significant differences')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Classification Report (for Classification Problems)\n",
    "\n",
    "Generate detailed classification reports for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate detailed classification reports (if classification problem)\n",
    "# if problem_type == 'classification':\n",
    "#     for name, model in models.items():\n",
    "#         print(f\"\\n{'='*50}\")\n",
    "#         print(f\"Classification Report - {name}\")\n",
    "#         print(f\"{'='*50}\")\n",
    "#         \n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         \n",
    "#         print(classification_report(y_test, y_pred))\n",
    "#         \n",
    "#         # Confusion matrix\n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         cm = confusion_matrix(y_test, y_pred)\n",
    "#         sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "#         plt.title(f'Confusion Matrix - {name}')\n",
    "#         plt.ylabel('True Label')\n",
    "#         plt.xlabel('Predicted Label')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Complexity Analysis\n",
    "\n",
    "Analyze the complexity and interpretability of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze model complexity\n",
    "# complexity_analysis = {\n",
    "#     'Model': [],\n",
    "#     'Interpretability': [],\n",
    "#     'Training_Complexity': [],\n",
    "#     'Prediction_Complexity': [],\n",
    "#     'Memory_Usage': [],\n",
    "#     'Hyperparameters': []\n",
    "# }\n",
    "\n",
    "# # Define complexity characteristics for each model\n",
    "# model_characteristics = {\n",
    "#     'Decision Tree': {\n",
    "#         'interpretability': 'High',\n",
    "#         'training_complexity': 'O(n*log(n)*m)',\n",
    "#         'prediction_complexity': 'O(log(n))',\n",
    "#         'memory_usage': 'Low',\n",
    "#         'hyperparameters': 'Few'\n",
    "#     },\n",
    "#     'SVM (RBF)': {\n",
    "#         'interpretability': 'Low',\n",
    "#         'training_complexity': 'O(n²) to O(n³)',\n",
    "#         'prediction_complexity': 'O(k*m)',\n",
    "#         'memory_usage': 'Medium',\n",
    "#         'hyperparameters': 'Medium'\n",
    "#     },\n",
    "#     # Add other models...\n",
    "# }\n",
    "\n",
    "# TODO: Create a comparison table\n",
    "# comparison_df = pd.DataFrame(model_characteristics).T\n",
    "# print(\"Model Complexity Analysis:\")\n",
    "# print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Bias-Variance Trade-off Analysis\n",
    "\n",
    "Analyze the bias-variance trade-off for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement bias-variance decomposition\n",
    "# This is a simplified analysis - for detailed bias-variance decomposition,\n",
    "# you might need specialized libraries like mlxtend\n",
    "\n",
    "# # Analyze variance across different train-test splits\n",
    "# from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# n_splits = 20\n",
    "# cv_splitter = ShuffleSplit(n_splits=n_splits, test_size=0.3, random_state=42)\n",
    "\n",
    "# model_variances = {}\n",
    "\n",
    "# for name, model in models.items():\n",
    "#     scores = []\n",
    "#     for train_idx, test_idx in cv_splitter.split(X_train):\n",
    "#         X_train_fold, X_test_fold = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "#         y_train_fold, y_test_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "#         \n",
    "#         model.fit(X_train_fold, y_train_fold)\n",
    "#         score = model.score(X_test_fold, y_test_fold)\n",
    "#         scores.append(score)\n",
    "#     \n",
    "#     model_variances[name] = {\n",
    "#         'mean': np.mean(scores),\n",
    "#         'variance': np.var(scores),\n",
    "#         'std': np.std(scores)\n",
    "#     }\n",
    "\n",
    "# # Plot bias-variance trade-off\n",
    "# variance_df = pd.DataFrame(model_variances).T\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(variance_df['variance'], variance_df['mean'], s=100)\n",
    "# for i, model in enumerate(variance_df.index):\n",
    "#     plt.annotate(model, (variance_df['variance'][i], variance_df['mean'][i]),\n",
    "#                  xytext=(5, 5), textcoords='offset points')\n",
    "# plt.xlabel('Variance')\n",
    "# plt.ylabel('Mean Performance')\n",
    "# plt.title('Bias-Variance Trade-off Analysis')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Learning Curves Comparison\n",
    "\n",
    "Compare how models perform with different amounts of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate learning curves for selected models\n",
    "# from sklearn.model_selection import learning_curve\n",
    "\n",
    "# # Select a subset of models for learning curve analysis\n",
    "# selected_models = ['Decision Tree', 'SVM (RBF)', 'Random Forest']\n",
    "\n",
    "# plt.figure(figsize=(15, 5))\n",
    "\n",
    "# for i, model_name in enumerate(selected_models):\n",
    "#     model = models[model_name]\n",
    "#     \n",
    "#     train_sizes, train_scores, test_scores = learning_curve(\n",
    "#         model, X_train, y_train, cv=5, n_jobs=-1, \n",
    "#         train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "#     )\n",
    "#     \n",
    "#     train_mean = np.mean(train_scores, axis=1)\n",
    "#     train_std = np.std(train_scores, axis=1)\n",
    "#     test_mean = np.mean(test_scores, axis=1)\n",
    "#     test_std = np.std(test_scores, axis=1)\n",
    "#     \n",
    "#     plt.subplot(1, 3, i+1)\n",
    "#     plt.plot(train_sizes, train_mean, 'o-', label='Training Score')\n",
    "#     plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "#     plt.plot(train_sizes, test_mean, 'o-', label='Validation Score')\n",
    "#     plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "#     \n",
    "#     plt.title(f'Learning Curve - {model_name}')\n",
    "#     plt.xlabel('Training Set Size')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Selection Criteria\n",
    "\n",
    "Establish comprehensive criteria for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a comprehensive model comparison table\n",
    "# model_comparison = pd.DataFrame({\n",
    "#     'Model': list(models.keys()),\n",
    "#     'CV_Score_Mean': [cv_results[model]['mean'] for model in models.keys()],\n",
    "#     'CV_Score_Std': [cv_results[model]['std'] for model in models.keys()],\n",
    "#     'Training_Time': [training_times[model] for model in models.keys()],\n",
    "#     'Prediction_Time': [prediction_times[model] for model in models.keys()]\n",
    "# })\n",
    "\n",
    "# # Add normalized scores for easier comparison\n",
    "# model_comparison['CV_Score_Normalized'] = (\n",
    "#     model_comparison['CV_Score_Mean'] / model_comparison['CV_Score_Mean'].max()\n",
    "# )\n",
    "# model_comparison['Speed_Score'] = (\n",
    "#     1 / (model_comparison['Training_Time'] + model_comparison['Prediction_Time'])\n",
    "# )\n",
    "# model_comparison['Speed_Score_Normalized'] = (\n",
    "#     model_comparison['Speed_Score'] / model_comparison['Speed_Score'].max()\n",
    "# )\n",
    "\n",
    "# # Calculate composite score (you can adjust weights based on your priorities)\n",
    "# performance_weight = 0.7\n",
    "# speed_weight = 0.3\n",
    "# model_comparison['Composite_Score'] = (\n",
    "#     performance_weight * model_comparison['CV_Score_Normalized'] +\n",
    "#     speed_weight * model_comparison['Speed_Score_Normalized']\n",
    "# )\n",
    "\n",
    "# # Sort by composite score\n",
    "# model_comparison = model_comparison.sort_values('Composite_Score', ascending=False)\n",
    "# print(\"Model Comparison Summary:\")\n",
    "# print(model_comparison.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Final Model Recommendation\n",
    "\n",
    "Make a final recommendation based on comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make final recommendation\n",
    "# best_model_name = model_comparison.iloc[0]['Model']\n",
    "# best_model = models[best_model_name]\n",
    "\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(f\"FINAL MODEL RECOMMENDATION: {best_model_name}\")\n",
    "# print(f\"{'='*60}\")\n",
    "\n",
    "# print(f\"\\nReasons for selection:\")\n",
    "# print(f\"1. Cross-validation score: {cv_results[best_model_name]['mean']:.4f} (+/- {cv_results[best_model_name]['std']*2:.4f})\")\n",
    "# print(f\"2. Training time: {training_times[best_model_name]:.4f} seconds\")\n",
    "# print(f\"3. Prediction time: {prediction_times[best_model_name]:.4f} seconds\")\n",
    "# print(f\"4. Composite score: {model_comparison.iloc[0]['Composite_Score']:.4f}\")\n",
    "\n",
    "# # Train the final model on full training data\n",
    "# final_model = best_model\n",
    "# final_model.fit(X_train, y_train)\n",
    "# final_predictions = final_model.predict(X_test)\n",
    "\n",
    "# # Final evaluation\n",
    "# if problem_type == 'classification':\n",
    "#     final_accuracy = accuracy_score(y_test, final_predictions)\n",
    "#     print(f\"\\nFinal test accuracy: {final_accuracy:.4f}\")\n",
    "# else:\n",
    "#     final_r2 = r2_score(y_test, final_predictions)\n",
    "#     final_mse = mean_squared_error(y_test, final_predictions)\n",
    "#     print(f\"\\nFinal test R²: {final_r2:.4f}\")\n",
    "#     print(f\"Final test MSE: {final_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Model Deployment Considerations\n",
    "\n",
    "Discuss considerations for deploying the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the final model and preprocessing steps\n",
    "# import joblib\n",
    "\n",
    "# # Save the model\n",
    "# joblib.dump(final_model, '../models/final_model.pkl')\n",
    "# print\"Final model saved to '../models/final_model.pkl'\")\n",
    "\n",
    "# # Create a simple prediction function\n",
    "# def make_prediction(input_data):\n",
    "#     \"\"\"\n",
    "#     Make prediction using the final model.\n",
    "#     \n",
    "#     Args:\n",
    "#         input_data: preprocessed input data\n",
    "#     \n",
    "#     Returns:\n",
    "#         prediction\n",
    "#     \"\"\"\n",
    "#     prediction = final_model.predict(input_data)\n",
    "#     return prediction\n",
    "\n",
    "# # Test the prediction function\n",
    "# sample_prediction = make_prediction(X_test.iloc[:1])\n",
    "# print(f\"Sample prediction: {sample_prediction[0]}\")\n",
    "# print(f\"Actual value: {y_test.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "Provide a comprehensive summary of the model comparison analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "**TODO: Summarize your key findings here:**\n",
    "\n",
    "1. **Best Performing Model:** [Model name and performance metrics]\n",
    "2. **Performance vs Complexity Trade-offs:** [Discussion of trade-offs]\n",
    "3. **Computational Efficiency:** [Training and prediction time analysis]\n",
    "4. **Statistical Significance:** [Results of significance tests]\n",
    "5. **Bias-Variance Analysis:** [Insights from bias-variance trade-off]\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "**TODO: Provide specific recommendations:**\n",
    "\n",
    "1. **Primary Recommendation:** [Best model for production use]\n",
    "2. **Alternative Options:** [Backup models and when to use them]\n",
    "3. **Deployment Considerations:** [Important factors for deployment]\n",
    "4. **Future Improvements:** [Suggestions for model enhancement]\n",
    "\n",
    "### Lessons Learned\n",
    "\n",
    "**TODO: Discuss lessons learned:**\n",
    "\n",
    "1. **Data Insights:** [What the comparison revealed about your data]\n",
    "2. **Algorithm Insights:** [Strengths and weaknesses of different approaches]\n",
    "3. **Methodology Insights:** [Lessons about model comparison process]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "1. Which model performed best overall and why do you think it was superior for this dataset?\n",
    "2. How did the computational efficiency of models compare, and how important is this for your use case?\n",
    "3. What role did statistical significance testing play in your model selection decision?\n",
    "4. How did the bias-variance trade-off manifest differently across the models?\n",
    "5. What would you do differently if you had to deploy this model in a production environment?\n",
    "6. How might the model comparison results change with a different dataset or problem domain?\n",
    "7. What additional models or techniques would you consider exploring in future iterations?\n",
    "\n",
    "**TODO: Answer the reflection questions above in markdown cells below.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}