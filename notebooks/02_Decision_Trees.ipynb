{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Decision Trees\n",
    "\n",
    "This notebook covers the implementation and analysis of Decision Tree algorithms for machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import the necessary libraries for Decision Tree implementation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load, Split and Preprocess Data\n",
    "\n",
    "First, load the dataset and split it into training, validation, and test sets (60/20/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(341, 30) (114, 30) (114, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"target\")\n",
    "\n",
    "# 60% train, 20% validation, 20% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, preprocess the data by scaling the features using `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Decision Tree Implementation\n",
    "\n",
    "Implement a basic Decision Tree model with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy: 0.909 ± 0.017\n",
      "test_precision: 0.934 ± 0.010\n",
      "test_recall: 0.921 ± 0.024\n",
      "test_roc_auc: 0.905 ± 0.016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = cross_validate(\n",
    "    dt, X_train_scaled, y_train,\n",
    "    cv=cv,\n",
    "    scoring=['accuracy', 'precision', 'recall', 'roc_auc'],\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "for metric in ['test_accuracy', 'test_precision', 'test_recall', 'test_roc_auc']:\n",
    "    print(f\"{metric}: {scores[metric].mean():.3f} ± {scores[metric].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Now we will analyze the feature importance of the trained Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst radius: 0.7110\n",
      "worst concave points: 0.1580\n",
      "texture error: 0.0390\n"
     ]
    }
   ],
   "source": [
    "dt.fit(X_train_scaled, y_train)\n",
    "importances = dt.feature_importances_\n",
    "\n",
    "# Sort features by importance\n",
    "indices = importances.argsort()[::-1]\n",
    "for idx in indices[:3]:\n",
    "    print(f\"{X_train.columns[idx]}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the three most important features are:\n",
    "- worst radius with a feature importance of 0.7110\n",
    "- worst concave points with a feature importance of 0.1580\n",
    "- texture error with a feature importance of 0.0390\n",
    "\n",
    "These three features together account for approximately 90.8% of the total feature importance, indicating that they play a significant role in the model's decision-making process.\n",
    "\n",
    "### Depth Analysis\n",
    "\n",
    "Analyze the effect of tree depth on model performance by training Decision Trees with varying depths and evaluating their accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 2: 0.947 ± 0.020\n",
      "Depth 4: 0.927 ± 0.009\n",
      "Depth 6: 0.921 ± 0.015\n",
      "Depth 8: 0.909 ± 0.017\n",
      "Depth 10: 0.909 ± 0.017\n"
     ]
    }
   ],
   "source": [
    "depths = [2, 4, 6, 8, 10]\n",
    "for d in depths:\n",
    "    dt = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    scores = cross_validate(dt, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    print(f\"Depth {d}: {scores['test_score'].mean():.3f} ± {scores['test_score'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the accuracy on the accuracy on the validation set is highest at a depth of 2, and decreases as the depth increases beyond that point. This suggests that a depth of 2 provides the best balance between model complexity and generalization to unseen data.\n",
    "\n",
    "Decision trees, if left unconstrained, will adapt itself to the training data, leading to overfitting. By limiting the depth of the tree, we can control its complexity and improve its ability to generalize to new data (Géron, p. 268, 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "We will now perform a grid search to find the optimal hyperparameters for the Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 2, 'min_samples_leaf': 5}\n",
      "Best CV accuracy: 0.9530264279624895\n",
      "Validation accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 7, None],\n",
    "    'min_samples_leaf': [1, 2, 4, 5, 10, 20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    dt, param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best CV accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_acc = grid_search.score(X_val_scaled, y_val)\n",
    "print(\"Validation accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Depth Analysis\n",
    "\n",
    "By analyzing the effect of the `max_depth` parameter on model performance, we can determine the optimal value for this parameter. The parameter controls the maximum depth of the tree, which can help prevent overfitting.\n",
    "\n",
    "We observe that the accuracy on the accuracy on the validation set is highest at a depth of 2, and decreases as the depth increases beyond that point. This suggests that a depth of 2 provides the best balance between model complexity and generalization to unseen data.\n",
    "\n",
    "#### Minimum Samples per Leaf Analysis\n",
    "\n",
    "By analyzing the effect of the `min_samples_leaf` parameter on model performance, we can determine the optimal value for this parameter. The parameter controls the minimum number of samples required to be at a leaf node, which can help prevent overfitting.\n",
    "\n",
    "Here we see that the Decision Tree model performs best when the `min_samples_leaf` parameter is set to five. This means that each leaf node in the tree must contain at least five samples, which helps to prevent overfitting and improve the model's generalization to unseen data. The accuracy of the model with this parameter setting is 0.953.\n",
    "\n",
    "## 4. Test the Model\n",
    "\n",
    "Finally, evaluate the best model on the test set to assess its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9035087719298246\n",
      "Test Precision: 0.9411764705882353\n",
      "Test Recall: 0.9014084507042254\n",
      "Test AUC: 0.9264657713724206\n"
     ]
    }
   ],
   "source": [
    "# Combine train + val\n",
    "X_combined = np.vstack((X_train_scaled, X_val_scaled))\n",
    "y_combined = np.hstack((y_train, y_val))\n",
    "\n",
    "best_dt = DecisionTreeClassifier(**grid_search.best_params_, random_state=42)\n",
    "best_dt.fit(X_combined, y_combined)\n",
    "\n",
    "y_pred = best_dt.predict(X_test_scaled)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Test Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"Test AUC:\", roc_auc_score(y_test, best_dt.predict_proba(X_test_scaled)[:, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
